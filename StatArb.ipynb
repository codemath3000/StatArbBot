{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M16KfCI8ALf"
      },
      "source": [
        "First, we need to run some basic initial commands and import necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvrPai5amM9m"
      },
      "outputs": [],
      "source": [
        "!rm *.csv.gz\n",
        "!pip install boto3\n",
        "!pip install botocore\n",
        "!pip install pandas\n",
        "!pip install gzip\n",
        "!pip install scipy\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "from scipy.stats import linregress\n",
        "from random import randint\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIZYbWCT8TO3"
      },
      "source": [
        "This is the backtesting function; we will need it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvTj8VTr78D-"
      },
      "outputs": [],
      "source": [
        "def run_backtest(merged_df: pd.DataFrame, m: float, b: float,\n",
        "                 spread_bps: float) -> pd.DataFrame:\n",
        "    merged_df = merged_df.sort_values(\"window_start\").reset_index(drop=True)\n",
        "    spr = spread_bps / 10000\n",
        "\n",
        "    prev_px_x = prev_px_y = None\n",
        "    prev_pos1 = prev_pos2 = 0\n",
        "    cum_pnl = 0.0\n",
        "    recs = []\n",
        "\n",
        "    for _, row in merged_df.iterrows():\n",
        "        ts = row[\"window_start\"]\n",
        "        px_x = row[\"open_x\"]\n",
        "        px_y = row[\"open_y\"]\n",
        "\n",
        "        if prev_px_x is None:\n",
        "            pos1 = pos2 = 0\n",
        "            trade_cost = 0.0\n",
        "            step_pnl = 0.0\n",
        "        else:\n",
        "            if px_y < px_x * m + b:\n",
        "                pos1, pos2 = -1 / px_x, 1 / px_y\n",
        "            else:\n",
        "                pos1, pos2 = 1 / px_x, -1 / px_y\n",
        "\n",
        "            step_pnl = (\n",
        "                prev_pos1 * (px_x - prev_px_x) +\n",
        "                prev_pos2 * (px_y - prev_px_y)\n",
        "            )\n",
        "\n",
        "            trade1 = abs(pos1 - prev_pos1)\n",
        "            trade2 = abs(pos2 - prev_pos2)\n",
        "\n",
        "            trade_cost = spr * (trade1 * px_x + trade2 * px_y)\n",
        "            step_pnl -= trade_cost\n",
        "\n",
        "            cum_pnl += step_pnl\n",
        "\n",
        "        recs.append({\"time\": ts, \"pnl\": cum_pnl})\n",
        "\n",
        "        prev_px_x, prev_px_y = px_x, px_y\n",
        "        prev_pos1, prev_pos2 = pos1, pos2\n",
        "\n",
        "    return pd.DataFrame(recs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu9LruP896wR"
      },
      "source": [
        "This sets up a connection to the S3 bucket containing the data. Note that this code currently uses Google Colab's secrets functionality to store the S3 credentials for Polygon's file-based API, but other approaches for secret storage can be easily substituted in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znKEEb2c8QuA"
      },
      "outputs": [],
      "source": [
        "session = boto3.Session(\n",
        "  aws_access_key_id=userdata.get('POLYGON_AWS_ID'),\n",
        "  aws_secret_access_key=userdata.get('POLYGON_AWS_SECRET'),\n",
        ")\n",
        "\n",
        "s3 = session.client(\n",
        "  's3',\n",
        "  endpoint_url='https://files.polygon.io',\n",
        "  config=Config(signature_version='s3v4'),\n",
        ")\n",
        "\n",
        "paginator = s3.get_paginator('list_objects_v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5OKlZ_Eh56g"
      },
      "source": [
        "Now, we can download the data and extract it from the gzip file.\n",
        "\n",
        "Note that, currently, it just takes in one day of data. That is due to compute constraints; it already takes about an hour to run this notebook on Google Colab, which is already pushing up against the bounds of practical usability. However, there is no reason why, in principle, more data couldn't be used given more compute, and it is set up to be extensible if you want to try adding more files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttNqRr5v8chy"
      },
      "outputs": [],
      "source": [
        "all_dataframes = []\n",
        "\n",
        "for prefix in [\"us_stocks_sip/minute_aggs_v1/2025/05/2025-05-30\"]:\n",
        "  for page in paginator.paginate(Bucket='flatfiles', Prefix=prefix):\n",
        "    for obj in page['Contents']:\n",
        "      print(\"Processing data input file: \" + obj['Key'])\n",
        "\n",
        "      bucket_name = 'flatfiles'\n",
        "\n",
        "      local_file_name = obj['Key'].split('/')[-1]\n",
        "\n",
        "      local_file_path = './' + local_file_name\n",
        "\n",
        "      s3.download_file(bucket_name, obj['Key'], local_file_path)\n",
        "      with gzip.open(local_file_path) as downloaded_file:\n",
        "        all_dataframes.append(pd.read_csv(local_file_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia6t8SCWnB89"
      },
      "source": [
        "If multiple days of data are used, this combines all of the dataframes together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9PGjARg8g11"
      },
      "outputs": [],
      "source": [
        "combined_df = pd.concat(all_dataframes, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbQgcAirmzPb"
      },
      "source": [
        "This goes through the data and separates it by ticker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGDCrw6e8jm4"
      },
      "outputs": [],
      "source": [
        "ticker_dataframes = {}\n",
        "for ticker in combined_df['ticker'].unique():\n",
        "  ticker_dataframes[ticker] = combined_df[combined_df['ticker'] == ticker].copy()\n",
        "  print(\"Completed initial processing of ticker: \" + str(ticker))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_RQXASPm37J"
      },
      "source": [
        "We won't be using combined_df or all_dataframes anymore, so we can get rid of them to free up memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufLGsZF48mBv"
      },
      "outputs": [],
      "source": [
        "del combined_df\n",
        "all_dataframes = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEc7A_Uvm6c3"
      },
      "source": [
        "This extracts the features that will be needed for the subsequent regression step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMM_zwob8zl1"
      },
      "outputs": [],
      "source": [
        "reg_dataframes = {}\n",
        "for ticker, df in ticker_dataframes.items():\n",
        "\n",
        "  reg_dataframes[ticker] = pd.DataFrame({\n",
        "      'open_price': df['open'],\n",
        "      'window_start': df['window_start']\n",
        "  })\n",
        "  print(\"Completed secondary processing of ticker: \" + str(ticker))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeylFjNqjw5V"
      },
      "source": [
        "This randomly generates a bunch of pairs for a pairs trading stat-arb strategy. Then, linear regressions are performed on each one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uySIBUr86Ek"
      },
      "outputs": [],
      "source": [
        "regression_results = {}\n",
        "\n",
        "all_tickers = list(reg_dataframes.keys())\n",
        "\n",
        "for randct in range(16384):\n",
        "    i = randint(0, len(all_tickers) - 1)\n",
        "    j = randint(0, len(all_tickers) - 1)\n",
        "    while j == i:\n",
        "      j = randint(0, len(all_tickers) - 1)\n",
        "    ticker1 = all_tickers[i]\n",
        "    ticker2 = all_tickers[j]\n",
        "\n",
        "    df1 = reg_dataframes[ticker1]\n",
        "    df2 = reg_dataframes[ticker2]\n",
        "\n",
        "    merged_df = pd.merge(df1, df2, on='window_start', how='inner')\n",
        "\n",
        "    if len(merged_df) > 32:\n",
        "      try:\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(merged_df['open_price_x'], merged_df['open_price_y'])\n",
        "\n",
        "        regression_results[(ticker1, ticker2)] = (abs(r_value), slope, intercept)\n",
        "      except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7gAHW8Xj-O5"
      },
      "source": [
        "Next, with all of the regressions complete, we can sort by correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7keGMjlQ9BTH"
      },
      "outputs": [],
      "source": [
        "regression_list = list(regression_results.items())\n",
        "sorted_regression_list = sorted(regression_list, key=lambda item: item[1][0], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rad8jUOkKGR"
      },
      "source": [
        "Before the strategy can be tested, a test data file must be obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNwCesUj9NGY"
      },
      "outputs": [],
      "source": [
        "for prefix in [\"us_stocks_sip/minute_aggs_v1/2025/06/2025-06-02\"]:\n",
        "  for page in paginator.paginate(Bucket='flatfiles', Prefix=prefix):\n",
        "    for obj in page['Contents']:\n",
        "      print(\"Processing data input file: \" + obj['Key'])\n",
        "\n",
        "      bucket_name = 'flatfiles'\n",
        "\n",
        "      local_file_name = obj['Key'].split('/')[-1]\n",
        "\n",
        "      local_file_path = './' + local_file_name\n",
        "\n",
        "      s3.download_file(bucket_name, obj['Key'], local_file_path)\n",
        "      with gzip.open(local_file_path) as downloaded_file:\n",
        "        all_dataframes.append(pd.read_csv(local_file_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ8NungwkdpX"
      },
      "source": [
        "Just like before, if there are multiple days, they must be combined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5ssMJWp9RzM"
      },
      "outputs": [],
      "source": [
        "combined_df = pd.concat(all_dataframes, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n5Fr9YWkh1Z"
      },
      "source": [
        "The test data then gets separated by ticker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOJWI-wY9XSd"
      },
      "outputs": [],
      "source": [
        "ticker_dataframes = {}\n",
        "for ticker in combined_df['ticker'].unique():\n",
        "  ticker_dataframes[ticker] = combined_df[combined_df['ticker'] == ticker].copy()\n",
        "  print(\"Completed test data processing of ticker: \" + str(ticker))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6am9epJkoKq"
      },
      "source": [
        "Again, combined_df and all_dataframes can now be safely deleted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3nqRjN39it7"
      },
      "outputs": [],
      "source": [
        "del combined_df\n",
        "all_dataframes = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWqW2U0jkuZ5"
      },
      "source": [
        "For each of the selected pairs, the strategy is backtested using the data from the test day. The range to select from in sorted_regression_list were determined empirically; I wanted to ensure that the pairs had a genuine, important correlation, so I excluded the final 5/8 of pairs, but I also noticed some weird behavior (and strongly negative PnL) with the pairs at the very front of the list, so I excluded those too.\n",
        "\n",
        "Also, note that, for financial reasons, the historical market data I have purchased access to only includes open and close prices and not bid-ask spread. Therefore, I am assuming a constant bid-ask spread for the purposes of the backtest; according to nasdaq.com, the average spread for S&P 500 stocks is 3.7 bp, so I am adding 0.5 onto that to get 4.2 bp as a pessimistic estimate to ensure a margin of safety."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epeW1q71xkAs"
      },
      "outputs": [],
      "source": [
        "backtest_results = {}\n",
        "for regression in sorted_regression_list[1024:6144]:\n",
        "  ticker1 = regression[0][0]\n",
        "  ticker2 = regression[0][1]\n",
        "  if (ticker1 not in ticker_dataframes) or (ticker2 not in ticker_dataframes):\n",
        "    continue\n",
        "\n",
        "  df1 = ticker_dataframes[ticker1]\n",
        "  df2 = ticker_dataframes[ticker2]\n",
        "\n",
        "  m = regression[1][1]\n",
        "  b = regression[1][2]\n",
        "\n",
        "  merged_df = pd.merge(\n",
        "    df1[[\"window_start\", \"open\"]],\n",
        "    df2[[\"window_start\", \"open\"]],\n",
        "    on=\"window_start\",\n",
        "    how=\"inner\",\n",
        "    suffixes=(\"_x\", \"_y\")\n",
        "  )\n",
        "\n",
        "  if merged_df.empty:\n",
        "    continue\n",
        "\n",
        "  backtest_results[(ticker1, ticker2)] = run_backtest(merged_df, m, b, 4.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTrOGwdzlHaL"
      },
      "source": [
        "The backtest results on a per-pair basis must be merged to get overall PnL data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFuCBn_s7qhi"
      },
      "outputs": [],
      "source": [
        "all_pair_pnls = []\n",
        "\n",
        "for pair, df in backtest_results.items():\n",
        "    s = df.set_index('time')['pnl'].astype(float)\n",
        "    all_pair_pnls.append(s)\n",
        "\n",
        "overall_pnl = (\n",
        "  pd.concat(all_pair_pnls, axis=1)\n",
        "    .sort_index()\n",
        "    .fillna(method='ffill')\n",
        "    .fillna(0.0)\n",
        "    .sum(axis=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRqjSvGwlrDY"
      },
      "source": [
        "The Sharpe ratio can be calculated from the overall PnL data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oc8Qlivem9W"
      },
      "outputs": [],
      "source": [
        "returns = overall_pnl.diff().dropna()\n",
        "\n",
        "ann_factor = (252 * 390) ** 0.5\n",
        "sharpe_ratio = returns.mean() / returns.std() * ann_factor\n",
        "\n",
        "print(f\"Sharpe ratio: {sharpe_ratio:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4d255Ykl3sS"
      },
      "source": [
        "Finally, the PnL graph can be displayed. Note that the timestamps are relative to Unix epoch, so they are not shown here to avoid visual clutter. However, if you want to see them anyway, feel free to comment out the `plt.xticks([])` line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a8i85IzepFt"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(overall_pnl.index, overall_pnl.values)\n",
        "plt.title(\"Cumulative P&L\")\n",
        "plt.xlabel(\"Time of Day\")\n",
        "plt.xticks([])\n",
        "plt.ylabel(\"Cumulative P&L\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}